{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f810ee72",
   "metadata": {},
   "source": [
    "## 0. Follow along of Andrew's video \n",
    "- Let's build GPT: from scratch, in code, spelled out.\n",
    "- https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8\n",
    "- Data used is Lord of the Rings trilogy book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f941a",
   "metadata": {},
   "source": [
    "## 1. Start Building a GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c055d99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  2579949\n"
     ]
    }
   ],
   "source": [
    "#### Read files\n",
    "DIRECTORY = \"data/lotr_books/\"\n",
    "files = [\"1_fellow.txt\", \"2_two_tower.txt\", \"3_return_of_the_king.txt\"]\n",
    "\n",
    "text = \"\"\n",
    "for file in files: \n",
    "    with open (DIRECTORY + file, 'r', encoding='utf-8') as f:\n",
    "        text += f.read() + \" \"\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b3f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Rings for the Elven-kings under the sky,\n",
      "               Seven for the Dwarf-lords in their halls of stone,\n",
      "            Nine for Mortal Men doomed to die,\n",
      "              One for the Dark Lord on his dark throne\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "               One Ring to rule them all, One Ring to find them,\n",
      "               One Ring to bring them all and in the darkness bind them\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "           \n",
      "FOREWORD\n",
      "\n",
      "This tale grew in the telling, until it became a history of the Great War of the Ring and included many glimpses of the yet more ancient history that preceded it. It was begun soon after _The Hobbit_ was written and before its publication in 1937; but I did not go on with this sequel, for I wished first to complete and set in order the mythology and legends of the Elder Days, which had then been taking shape for some years. I desired to do this for my own satisfaction, and I had little hope that other people \n"
     ]
    }
   ],
   "source": [
    "print (text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ba1d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !\"'()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ_`abcdefghijklmnopqrstuvwxyzÉÓáâäéêëíîóôúû–\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87eddf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87e3311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 62, 72, 59, 59, 2, 72, 63, 68, 61, 73, 3]\n",
      "Three rings!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([ itos[i] for i in l])\n",
    "\n",
    "print (encode(\"Three rings!\"))\n",
    "print (decode(encode(\"Three rings!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce5c98",
   "metadata": {},
   "source": [
    "### 2. Load text in Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b6b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2579949]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "### Store text in Pytorch \n",
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print (data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a058df8",
   "metadata": {},
   "source": [
    "### 3. Init train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b7d262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2579949\n"
     ]
    }
   ],
   "source": [
    "print (len(data))\n",
    "n = (int)(0.9 * len(data))\n",
    "train_data = data[0:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45eab454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 62, 72, 59, 59,  2, 44, 63, 68])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da715bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Ri\n",
      "when input is T the target: h\n",
      "when input is Th the target: r\n",
      "when input is Thr the target: e\n",
      "when input is Thre the target: e\n",
      "when input is Three the target:  \n",
      "when input is Three  the target: R\n",
      "when input is Three R the target: i\n",
      "when input is Three Ri the target: n\n"
     ]
    }
   ],
   "source": [
    "# Three Rings for the Elven-kings u\n",
    "x = text[:block_size]\n",
    "y = text[1 :block_size + 1]\n",
    "print (text[:block_size])\n",
    "for t in range(block_size):\n",
    "    print(f\"when input is {x[:t+1]} the target: {y[t]}\")    \n",
    "    \n",
    "#     print ((x[:t+1]), y[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fac9de",
   "metadata": {},
   "source": [
    "### Setting up train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9671c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46, 62, 72, 59, 59,  2, 44, 63])\n",
      "when input is tensor([46]) the target: 62\n",
      "when input is tensor([46, 62]) the target: 72\n",
      "when input is tensor([46, 62, 72]) the target: 59\n",
      "when input is tensor([46, 62, 72, 59]) the target: 59\n",
      "when input is tensor([46, 62, 72, 59, 59]) the target: 2\n",
      "when input is tensor([46, 62, 72, 59, 59,  2]) the target: 44\n",
      "when input is tensor([46, 62, 72, 59, 59,  2, 44]) the target: 63\n",
      "when input is tensor([46, 62, 72, 59, 59,  2, 44, 63]) the target: 68\n"
     ]
    }
   ],
   "source": [
    "## Setting up the train and test \n",
    "x = train_data[:block_size]\n",
    "y = train_data[1 :block_size + 1]\n",
    "print (x)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809e7a9",
   "metadata": {},
   "source": [
    "### Batch train/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6b00a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "torch.Size([4, 8])\n",
      "tensor([[66,  2, 69, 75, 72,  2, 60, 69],\n",
      "        [32, 72, 69, 58, 69,  2, 69, 68],\n",
      "        [59, 55, 73, 74, 11,  2, 46, 62],\n",
      "        [74, 69,  2, 74, 62, 59,  2, 72]])\n",
      "output:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 2, 69, 75, 72,  2, 60, 69, 69],\n",
      "        [72, 69, 58, 69,  2, 69, 68, 66],\n",
      "        [55, 73, 74, 11,  2, 46, 62, 59],\n",
      "        [69,  2, 74, 62, 59,  2, 72, 69]])\n",
      "---\n",
      "when input is [66] the target: 2\n",
      "when input is [66, 2] the target: 69\n",
      "when input is [66, 2, 69] the target: 75\n",
      "when input is [66, 2, 69, 75] the target: 72\n",
      "when input is [66, 2, 69, 75, 72] the target: 2\n",
      "when input is [66, 2, 69, 75, 72, 2] the target: 60\n",
      "when input is [66, 2, 69, 75, 72, 2, 60] the target: 69\n",
      "when input is [66, 2, 69, 75, 72, 2, 60, 69] the target: 69\n",
      "when input is [32] the target: 72\n",
      "when input is [32, 72] the target: 69\n",
      "when input is [32, 72, 69] the target: 58\n",
      "when input is [32, 72, 69, 58] the target: 69\n",
      "when input is [32, 72, 69, 58, 69] the target: 2\n",
      "when input is [32, 72, 69, 58, 69, 2] the target: 69\n",
      "when input is [32, 72, 69, 58, 69, 2, 69] the target: 68\n",
      "when input is [32, 72, 69, 58, 69, 2, 69, 68] the target: 66\n",
      "when input is [59] the target: 55\n",
      "when input is [59, 55] the target: 73\n",
      "when input is [59, 55, 73] the target: 74\n",
      "when input is [59, 55, 73, 74] the target: 11\n",
      "when input is [59, 55, 73, 74, 11] the target: 2\n",
      "when input is [59, 55, 73, 74, 11, 2] the target: 46\n",
      "when input is [59, 55, 73, 74, 11, 2, 46] the target: 62\n",
      "when input is [59, 55, 73, 74, 11, 2, 46, 62] the target: 59\n",
      "when input is [74] the target: 69\n",
      "when input is [74, 69] the target: 2\n",
      "when input is [74, 69, 2] the target: 74\n",
      "when input is [74, 69, 2, 74] the target: 62\n",
      "when input is [74, 69, 2, 74, 62] the target: 59\n",
      "when input is [74, 69, 2, 74, 62, 59] the target: 2\n",
      "when input is [74, 69, 2, 74, 62, 59, 2] the target: 72\n",
      "when input is [74, 69, 2, 74, 62, 59, 2, 72] the target: 69\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1001)\n",
    "\n",
    "batch_size = 4 #  how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "# Batch size * Block size\n",
    "def get_batch(split=\"train\"):\n",
    "    \n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "#     print (ix)\n",
    "    x = torch.stack([data[i:i+ block_size ] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + 1 + block_size]  for i in ix])\n",
    "    return x, y\n",
    "    \n",
    "xb, yb = get_batch()\n",
    "print (\"input:\")\n",
    "print (xb.shape)\n",
    "print (xb)\n",
    "print (\"output:\")\n",
    "print (yb.shape)\n",
    "print (yb)\n",
    "print (\"---\")\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size): # time dimension \n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6d1c7",
   "metadata": {},
   "source": [
    "## Bi-gram language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ef9f6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 96])\n",
      "tensor(4.7629, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "\tqmNNÉo8yä*î?wÉ(PóE\n",
      "a\t=c3?'8êeUkeBpV?J'X0I3ÉQI––Xvî1rún_(QJäûh2uiDkFá–cI7G2\n",
      "p=h/5ë?lGÓi8h;:w:2P5oYTc.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1001)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1001)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "#             print (logits.shape, end=\", \")\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)         \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)      \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "\n",
    "print(loss)\n",
    "print ()\n",
    "print (decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100).tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9b62c9",
   "metadata": {},
   "source": [
    "### Optimizer and Loss : Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f675a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "843d1119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.507620334625244\n",
      "2.495558023452759\n",
      "2.462411403656006\n",
      "2.532461643218994\n",
      "2.514556646347046\n",
      "2.5485453605651855\n",
      "2.524698495864868\n",
      "2.4224276542663574\n",
      "2.415363311767578\n",
      "2.3687758445739746\n",
      "2.505344867706299\n",
      "2.5898702144622803\n",
      "2.4531447887420654\n",
      "2.5582571029663086\n",
      "2.4428608417510986\n",
      "2.4471120834350586\n",
      "2.390035629272461\n",
      "2.4536070823669434\n",
      "2.3848578929901123\n",
      "2.382842779159546\n",
      "2.417722702026367\n",
      "2.4070119857788086\n",
      "2.511599540710449\n",
      "2.3814377784729004\n",
      "2.3844478130340576\n",
      "2.476498603820801\n",
      "2.5566766262054443\n",
      "2.4394633769989014\n",
      "2.3513784408569336\n",
      "2.515833616256714\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(3000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps == 0 or steps % 100 == 0:\n",
    "        print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59bc0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRan0-tMid m Gere ce id heng tereile ouid st un `! om rofathak at slureg hereve waseristhesu, im, d  roume ad lÉochs finghas=re th ak an'stid tr. o re it althofthe ro AZ! m In lag sive NTOf  Tht f se, tlvedea Fä41und '1HáZ=xEê2éSth oumeloubor. oats Weaswola Thougends. Gay H\"\n",
      " gh f parot Whe aly tha ldofathed allllinde ffathaupe ou `Thachesan oly ogh d me cke mansaf r. Bic.'tofono Lôâô/ronopeeindact d, p cooYore re baghr at thoimsast,'  th2m. Sasesmy theemok xûngh ghe s rm ad  hale d. hit a an FSi\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5eef4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text[10020:10130]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
